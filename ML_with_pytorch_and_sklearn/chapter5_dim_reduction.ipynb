{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction: PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key take-aways\n",
    "- PCA is highly sensitive to data scaling\n",
    "- It finds a basis of orthogonal vectors for the input data that are uncorrelated with each other\n",
    "- The orthogonal don't have to be independent however!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of PCA is to reduce the dimensionality of vectors $\\pmb{x}\\in\\mathbb{R}^D$ to a low-dimensional latent space $\\pmb{z}\\in\\mathbb{R}^L$, through a linear orthogonal projection on the subspace that keeps the most variance of $\\pmb{x}$. For an input dataset of $N$ examples, we construct the design matrix $X\\in\\mathbb{R}^{N\\times D}$. Intuitively one can already understand that there might be a lot of redundant information in this matrix when there are correlated variables. PCA will extract these *linear* relationships and reduce the dimensionality of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to derive the (inverse) projection matrix $W\\in\\mathbb{R}^{D\\times L}$ is to minimize the reconstruction loss: \n",
    "$$ \\mathcal{L}(W, Z) = \\frac{1}{N}\\sum_{n=1}^N\\|\\pmb{x}_n-W\\pmb{z}_n\\|^2$$\n",
    "where we are approximating each $\\pmb{x}_n$ by $\\hat{\\pmb{x}}_n=W\\pmb{z}_n$. W then consists of the $L$ principal components: $W = \\begin{bmatrix}\\pmb{w}_1 & ... & \\pmb{w}_L \\end{bmatrix}$ and $\\pmb{z}_n$ are the *scores* for example $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, let us start by estimating the best 1d solution $\\pmb{w}_1\\in\\mathbb{R}^D$ and its associated scores $\\pmb{z}_1\\in\\mathbb{R}^N$: \n",
    "$$\\mathcal{L}(\\pmb{w}_1, \\pmb{z}_1) = \\frac{1}{N}\\sum_{i=1}^N (\\pmb{x}_i-z_{i1}\\pmb{w}_1)^T(\\pmb{x}_i-z_{i1}\\pmb{w}_1) = \\frac{1}{N}\\sum_{i=1}^N \\pmb{x}_i^T\\pmb{x}_i -2z_{i1}\\pmb{w}_1^T\\pmb{x}_i + z_{i1}^2\\pmb{w}_1^T\\pmb{w}_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\pmb{w}_1^T\\pmb{w}_1 = 1$ because of the orthonormality condition, and we can further calculate the values of $z_{i1}$ for which $\\mathcal{L}$ is minimized: \n",
    "$$ \\frac{\\partial}{\\partial z_{in}}\\mathcal{L}(\\pmb{w}_1, \\pmb{z}_1) = -2\\pmb{w}_1^T\\pmb{x}_i + 2*z_{i1} \\rightarrow z_{i1} = \\pmb{w}_1^T\\pmb{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the optimal embedding is obtained by orthogonally projecting the data onto $\\pmb{w}_1$. Substituting this in the loss function, which is now only dependent on $\\pmb{w}_1$: \n",
    "$$\\mathcal{L}(\\pmb{w}_1) = \\frac{1}{N}\\sum_{i=1}^N \\pmb{x}_i^T\\pmb{x}_i -2(\\pmb{w}_1^T\\pmb{x}_i)(\\pmb{w}_1^T\\pmb{x}_i) + (\\pmb{w}_1^T\\pmb{x}_i)^2 = \\frac{1}{N}\\sum_{i=1}^N \\pmb{x}_i^T\\pmb{x}_i - (\\pmb{w}_1^T\\pmb{x}_i)^2 = \\text{const} - \\pmb{w}_1^T\\pmb{x} \\pmb{x}^T\\pmb{w}_1 = \\text{const} - \\pmb{w}_1^T\\hat{\\Sigma} \\pmb{w}_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lagrangian subject to the constraint that $\\pmb{w}^T\\pmb{w}=1$ then becomes: \n",
    "$$ \\tilde{\\mathcal{L}}(\\pmb{w}_1) = \\pmb{w}_1^T\\hat{\\Sigma} \\pmb{w}_1 + \\lambda_1(\\pmb{w}_1^T\\pmb{w}_1-1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lagrangian is maximized when \n",
    "$$\\frac{\\partial}{\\partial \\pmb{w}_1}\\tilde{\\mathcal{L}}(\\pmb{w_1}) = 2\\hat{\\Sigma}\\pmb{w}_1 -2\\lambda_1\\pmb{w}_1=0 \\rightarrow \\hat{\\Sigma}\\pmb{w}_1 = \\lambda_1\\pmb{w}_1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore $\\pmb{w}_1$ is the eigenvector corresponding to the largest eigenvalue of the empirical covariance matrix $\\Sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Probabilistic interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can be interpreted in a probabilistic setting through the lens of Factor Analysis. In such a setting, it is assumed that $\\pmb{x}$ is a linear transformation of a normally distributed random variable $\\pmb{z}$ of lower dimensionality: $p(\\pmb{z}) = \\mathcal{N}(\\pmb{z}|\\pmb{0},\\pmb{I})$. The conditional model is then: $p(\\pmb{x}|\\pmb{z}) = \\mathcal{N}(\\pmb{x}|W\\pmb{x}+\\pmb{\\mu},\\pmb{\\Psi})$. The model for $\\pmb{x}$ then becomes: \n",
    "$$ p(\\pmb{x}) = \\int_{\\pmb{z}}p(\\pmb{x}|\\pmb{z})p(\\pmb{z})d\\pmb{z} = \\mathcal{N}(\\pmb{x}|\\pmb{\\mu}, WW^T+\\Psi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PCA, W is enforced to be orthonormal such that $\\Psi = \\sigma^2\\mathbb{I}$. The log-likelihood of the data under this distribution can be calculated and the MLE of the parameters $\\sigma$ and $W$ can then be found. For $\\sigma\\rightarrow 0$, this solution collapses to the regular PCA solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the implementation:\n",
    "- eig returns the eigenvectors as vectors next to each other: $[v_1 ... v_k]$, which are the principal components\n",
    "- the scikit-learn PCA class stores the components row-wise however\n",
    "- The explained_variance_ are just the eigenvalues of the covariance matrix\n",
    "- It DOES matter that the covariance matrix is calculated by deviding by (n-1) instead of n\n",
    "- You can just multiply on the other side to go from x to z: W^T * (W*x) = W^T * z = x since W^T W = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.linalg import svd, eig\n",
    "\n",
    "class myPCA:\n",
    "    \"\"\" My own implementation of PCA, which follows the scikit-learn API\"\"\"\n",
    "    def __init__(self, n_components=None, whiten=False) -> None:\n",
    "        self.n_components = n_components\n",
    "        self.whiten = whiten\n",
    "    def fit(self, X: np.ndarray):\n",
    "        assert len(X.shape) == 2\n",
    "        self.n_samples_, self.n_features_ = X.shape\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        Xc = X - self.mean_ \n",
    "        S = Xc.T @ Xc / (self.n_samples_-1)\n",
    "        S = np.cov(X.T)\n",
    "        _, _, Vt = svd(Xc, full_matrices=False)\n",
    "        assert S.shape == (self.n_features_, self.n_features_)\n",
    "        lambdas, vs = eig(S)\n",
    "        sorting = np.argsort(lambdas)[::-1]\n",
    "        self.explained_variance_ = lambdas[sorting]\n",
    "        self.explained_variance_ratio_ = self.explained_variance_ / sum(self.explained_variance_)\n",
    "        self.components_ = vs[:, sorting].T  # The scikit-learn API puts the components in rows!\n",
    "        \n",
    "\n",
    "    def get_covariance(self):\n",
    "        \"\"\"Compute data covariance with the generative model.\"\"\"\n",
    "        pass \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply dimensionality reduction to X.\"\"\"\n",
    "        return (X-self.mean_) @ self.components_.T  # z = W^T * x\n",
    "    def inverse_transform(self, Z):\n",
    "        \"\"\"Transform data back to original space.\"\"\"\n",
    "        return self.components_ @ Z  # x=W*z \n",
    "    def score(self, X):\n",
    "        \"\"\"Return the average log-likelihood of all samples.\"\"\"\n",
    "        pass \n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Return the log likelihood of each sample.\"\"\"\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>class_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>class_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>class_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>class_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>class_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline   target  \n",
       "0                          3.92   1065.0  class_0  \n",
       "1                          3.40   1050.0  class_0  \n",
       "2                          3.17   1185.0  class_0  \n",
       "3                          3.45   1480.0  class_0  \n",
       "4                          2.93    735.0  class_0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "wine = datasets.load_wine()\n",
    "wine_df = pd.DataFrame(data=wine.data, columns=wine['feature_names']) \n",
    "wine_df['target'] = pd.Series(wine['target_names'][wine['target']], dtype=\"category\")\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wine_df.drop('target', axis=1).values, wine_df['target'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first principal component: \n",
      "[ 1.64674979e-03 -6.68466219e-04  1.85354140e-04 -5.12219565e-03\n",
      "  1.84956010e-02  1.01414919e-03  1.47163437e-03 -1.34844500e-04\n",
      "  5.33631358e-04  2.31938304e-03  1.50302509e-04  7.17936506e-04\n",
      "  9.99809516e-01]\n",
      "[-1.64674979e-03  6.68466219e-04 -1.85354140e-04  5.12219565e-03\n",
      " -1.84956010e-02 -1.01414919e-03 -1.47163437e-03  1.34844500e-04\n",
      " -5.33631358e-04 -2.31938304e-03 -1.50302509e-04 -7.17936506e-04\n",
      " -9.99809516e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "skpca = PCA(n_components=3)\n",
    "mypca = myPCA()\n",
    "skpca.fit(X_train)\n",
    "mypca.fit(X_train)\n",
    "print(\"The first principal component: \")\n",
    "print(skpca.components_[0,:])\n",
    "print(mypca.components_[0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.47820042   6.4271982    1.29776714  -0.73965734  -6.00940331\n",
      "  -6.80326595 -20.41467372  -6.83644349   4.60658186 -15.51428216\n",
      "  59.50300987   0.59737902  -0.18589483  23.40864747   3.87981494\n",
      " -14.36678885   3.39020368  19.76266928  28.63978721  -2.57933646\n",
      "  26.39802441   6.25607049  23.83262819  -5.15350197   2.52981539\n",
      "  -8.57129744 -12.9542068   -0.6057908    9.22987363  -0.35615751\n",
      "  12.03944015  -8.09515335  10.41818156  -9.2648081   -1.93327944\n",
      "   4.84677119  -7.50184558   5.31686501  -7.68614472  -9.46455766\n",
      "   6.07103073  -3.80909118   5.47667016  -9.2292848   52.43190883]\n",
      "[ -9.47820042  -6.4271982   -1.29776714   0.73965734   6.00940331\n",
      "   6.80326595  20.41467372   6.83644349  -4.60658186  15.51428216\n",
      " -59.50300987  -0.59737902   0.18589483 -23.40864747  -3.87981494\n",
      "  14.36678885  -3.39020368 -19.76266928 -28.63978721   2.57933646\n",
      " -26.39802441  -6.25607049 -23.83262819   5.15350197  -2.52981539\n",
      "   8.57129744  12.9542068    0.6057908   -9.22987363   0.35615751\n",
      " -12.03944015   8.09515335 -10.41818156   9.2648081    1.93327944\n",
      "  -4.84677119   7.50184558  -5.31686501   7.68614472   9.46455766\n",
      "  -6.07103073   3.80909118  -5.47667016   9.2292848  -52.43190883]\n"
     ]
    }
   ],
   "source": [
    "print(skpca.transform(X_test)[:,1])\n",
    "print(mypca.transform(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.98292699e-01 1.52483167e-03 1.01618221e-04]\n",
      "[9.98292699e-01 1.52483167e-03 1.01618221e-04 5.18861180e-05\n",
      " 1.24410004e-05 9.46696498e-06 3.00790091e-06 1.42816676e-06\n",
      " 1.11531559e-06 7.62138779e-07 4.08724879e-07 2.46288634e-07\n",
      " 8.84364932e-08]\n"
     ]
    }
   ],
   "source": [
    "# The explained variance doesn't change when a different number of components is used\n",
    "print(skpca.explained_variance_ratio_)\n",
    "print(mypca.explained_variance_ratio_.real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.97881657 -0.05072107  0.42971562  0.30813218]\n",
      " [-1.49511106  1.19490758 -0.71988722  0.64025099]\n",
      " [-0.76254381  0.74116086 -1.10593211 -0.50357508]]\n",
      "[[ 0.42971562 -0.05072107 -0.97881657]\n",
      " [-0.71988722  1.19490758 -1.49511106]\n",
      " [-1.10593211  0.74116086 -0.76254381]]\n"
     ]
    }
   ],
   "source": [
    "test = np.random.randn(3,4)\n",
    "print(test)\n",
    "print(test[:, [2, 1, 0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
