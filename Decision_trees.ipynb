{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjyo8CEr91wEoZx6NcCPM7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomS55555/Machine-learning/blob/main/Decision_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning & Inductive inference\n",
        "\n",
        "## Chapter 2: Decision trees\n",
        "\n",
        "### What are they? \n",
        "Classification and regression trees (CART) are defined by recursively partitioning the input space, and defining a local model in each resulting region of the input space. The model can be represented by a tree, with one leaf per region.  \n",
        "The tree consists of a set of nested decision rules. At each node $i$, a specific feature dimension $d_i$ of the input vector $\\mathbf{x}$ is compared to a threshold value $t_i$ and the input is then passed down to the left or to the right branch, depending on whether it is above or below the threshold.  \n",
        "A regression tree on a dataset $\\mathcal{D}=\\{\\mathbf{x}_i,y_i\\}_{i=1}^N$ can then be defined as \n",
        "\\begin{equation}\n",
        "f(\\mathbf{x}; ð›‰ ) = \\sum_{j=1}^J w_j \\mathbb{I}(\\mathbf{x}\\in R_j)\n",
        "\\end{equation} \n",
        "where $J$ is the amount of leaf nodes and each $w_j$ is the average value of all examples classified in region $R_j$ such that $ð›‰=\\{(R_j,w_j):j=1:J\\}$ and $$w_j = \\frac{\\sum_{n=1}^Ny_n\\mathbb{I}(\\mathbf{x}\\in R_j)}{\\sum_{n=1}^N\\mathbb{I}(\\mathbf{x}\\in R_j)}$$  \n",
        "For classification problems, the leaves contain distributions over the labels, rather than just the mean response.  \n",
        "### How to fit them?\n",
        "To fit the model, a loss function needs to be defined:\n",
        "\\begin{equation}\n",
        "\\mathcal{L}(ð›‰) = \\sum_{n=1}^N\\ell(y_n, f(\\mathbf{x}_n,ð›‰)) = \\sum_{j=1}^J\\sum_{\\mathbf{x}_n\\in R_j} \\ell(y_i,w_j)\n",
        "\\end{equation}\n",
        "This loss is not differentiable because the discrete tree structure needs to be learned. Furthermore finding the optimal partitioning of the data is NP-complete (non-deterministic polynomial time). The standard procedure is to iteratively grow the tree. It works as follows:   \n",
        "* Each node $i$ a reached by a subset of the dataset: $\\mathcal{D}_i=\\{\\mathbf{x}_i,y_i\\}_{i=1}^{N_i}$ which is split in the node into a left dataset $\\mathcal{D}_i^L(j,t)=\\{(\\mathbf{x}_n,y_n)\\in N_i:y_{n,j}\\leq t\\}$ and a right dataset $\\mathcal{D}_i^R(j,t)=\\{(\\mathbf{x}_n,y_n)\\in N_i:y_{n,j} > t\\}$. The best feature dimension $j_i$ and the best feature value $t_i$ is chosen as follows:\n",
        "\\begin{equation}\n",
        "(j_i, t_i) = \\arg \\min_{j\\in \\{1 \\hdots D\\}}\\min_{t\\in\\mathcal{T}_j}{\\frac{|\\mathcal{D}_i^L(j,t)|}{|\\mathcal{D}|}c(\\mathcal{D}_i^L(j,t))+\\frac{|\\mathcal{D}_i^R(j,t)|}{|\\mathcal{D}|}c(\\mathcal{D}_i^R(j,t))}\n",
        "\\end{equation}\n",
        "* The set of possible thresholds $\\mathcal{T}_j$ for each feature $j$ can be obtained by sorting the unique values of $\\{x_{nj}\\}$.\n",
        "* The cost function itself can be chosen for regression for example the mean squared error: $$ c(\\mathcal{D}_i) = \\frac{1}{|\\mathcal{D}_i|}\\sum_{n\\in \\mathcal{D}_i} (y_n-\\bar{y})^2 $$\n",
        "* For classification, we first compute the empirical distribution over class labels at each node:\n",
        "$$ \\hat{\\pi}_{ic}=\\frac{1}{\\mathcal{D_i}}\\sum_{n\\in\\mathcal{D}_i}\\mathbb{I}(y_n=c)$$\n",
        "Given this, we can then compute the expected error rate (Gini index):\n",
        "$$G_i =  \\sum_{c=1}^C\\hat{\\pi}_{ic}(1-\\hat{\\pi}_{ic})=1-\\sum_{c=1}^C\\hat{\\pi}_{ic}^2$$"
      ],
      "metadata": {
        "id": "mWygzWDz0uHV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2XtMP4C4Txs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}