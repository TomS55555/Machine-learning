{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will explore both the statistics and implementation of Regression models. I will focus on linear regression, but the same methods should apply to other forms of regression such as with decision trees, or neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, a regression model is a model that tries to predict a target random variable $Y$, given a set of $p$ predictor (feature) variables $\\pmb{X} = \\{X_i\\}_{i=1}^p$. This can be extended to multivariate regression if mutiple variables variables have to be predicted. However I will not consider this case in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset $\\mathcal{D} = \\{\\pmb{x}_i,y_i\\}_{i=1}^{D}$ is obtained from the (true) data generating distribution $p_{X,Y}^{true}(x,y)$, and we will use this dataset to train and test a model on. We will therefore devide the dataset into a training, and test set: $\\mathcal{D}_{train}$, $\\mathcal{D}_{test}$. Those can be used for training and testing a model respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.579818</td>\n",
       "      <td>2.769459</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.430783</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994252</td>\n",
       "      <td>3.319626</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.510826</td>\n",
       "      <td>2.691243</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.203973</td>\n",
       "      <td>3.282789</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.751416</td>\n",
       "      <td>3.432373</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371564</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45      lpsa  \\\n",
       "1 -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0 -0.430783   \n",
       "2 -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0 -0.162519   \n",
       "3 -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20 -0.162519   \n",
       "4 -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0 -0.162519   \n",
       "5  0.751416  3.432373   62 -1.386294    0 -1.386294        6      0  0.371564   \n",
       "\n",
       "  train  \n",
       "1     T  \n",
       "2     T  \n",
       "3     T  \n",
       "4     T  \n",
       "5     T  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset_df = pd.read_csv('https://raw.githubusercontent.com/probml/probml-data/main/data/prostate/prostate.csv', sep='\\t', index_col=0)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45      lpsa\n",
      "1 -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0 -0.430783\n",
      "2 -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0 -0.162519\n",
      "3 -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20 -0.162519\n",
      "4 -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0 -0.162519\n",
      "5  0.751416  3.432373   62 -1.386294    0 -1.386294        6      0  0.371564\n",
      "Train, test dataset size:  67 30\n"
     ]
    }
   ],
   "source": [
    "# split in training and test set\n",
    "dataset_train = dataset_df.loc[dataset_df['train']=='T'].drop(['train'], axis=1)\n",
    "dataset_test = dataset_df.loc[dataset_df['train']=='F'].drop(['train'], axis=1)\n",
    "print(dataset_train.head())\n",
    "print(\"Train, test dataset size: \", len(dataset_train), len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    97.000000\n",
       "mean      6.752577\n",
       "std       0.722134\n",
       "min       6.000000\n",
       "25%       6.000000\n",
       "50%       7.000000\n",
       "75%       7.000000\n",
       "max       9.000000\n",
       "Name: gleason, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.gleason.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### PART 1: the MLE estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, the generative distribution $p_{X,Y}(x,y)$ is not modeled directly; but the conditional distribution instead: $p_{Y|X}(y|\\pmb{x})$. This is easier to model than the generative distribution and apparently achieves better predictions. It is a distribution over $Y$, since $X$ is considered given. In normal linear regression, the model is a normal distribution where the mean is predicted as a linear function of $X$ and the variance is constant (homoscedasticity). \n",
    "$$ p_{Y|X}(y|\\pmb{x}, \\pmb{\\theta}) = \\mathcal{N}(\\pmb{w}^T\\pmb{x}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\bigg(\\frac{-(y-\\pmb{w}^T\\pmb{x})^2}{2\\sigma^2}\\bigg) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once such a model is fitted, it can make predictions $\\hat{y} = \\pmb{w}^T\\pmb{x}$ for new data, which can be tested with the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to find the parameters $\\pmb{\\theta}=\\pmb{w}$ that fit the dataset, is to find the parameters that maximize the likelihood of obtaining the training dataset $\\mathcal{D}_{train}$. This is the maximum likelihood estimate and it is the argmax of the likelihood function $L(\\mathcal{D}_{train}, \\pmb{w}) = \\prod_{i=1}^D \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\bigg(\\frac{-(y_i-\\pmb{w}^T\\pmb{x_i})^2}{2\\sigma^2}\\bigg)$, which is equivalant to minimizing the negative log likelihood (NLL) because the likelihood is a monotone function. Therefore the MLE of the parameters $\\pmb{w}$ is given by: \n",
    "\n",
    "$$\\pmb{w}^{MLE} = \\argmin \\text{NLL} = \\argmin \\sum_{i=1}^D (y_i-\\pmb{w}^T\\pmb{x})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLE thus minimizes the Residual Sum of Squares RSS=$\\sum_{i=1}^D (y_i-\\pmb{w}^T\\pmb{x})^2$. This can be compared to the Total Sum of Squares TSS=$\\sum_{i=1}^D (y_i-\\bar{y})^2$, which is just the variance of the targets of the dataset, and can be seen as a prediction with the mean $\\bar{y}=\\frac{1}{D}\\sum_{i=1}^Dy_i$ of the targets (therefore not using $X$ at all). The \\textbf{coefficient of determination}, denoted by $R^2$ measures this difference: $$ R^2 = 1-\\frac{RSS}{TSS}$$ \n",
    "If the prediction is perfect, i.e. RSS = 0, $R^2$ will be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: the MAP estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLE can easily result in overfitting, since we are exposing the model only to the training dataset. In a bayesian setting however, we can add some 'prior' information that the weights should be close to zero and only depart from zero when there is (strong) evidence to do so. Ridge regression assumes a gaussian prior for the weights: $p(\\pmb{w}) = \\mathcal{N}(0, \\lambda^{-1}\\mathbb{I})$, where $\\lambda$ is the 'strength' of our belief. This is called $\\mathcal{l}_2$ regularization. The MAP is the maximum of the posterior distribution which equals:\n",
    "$$ \\pmb{w}^{\\text{MAP}} = \\argmax_{\\pmb{w}} p(\\pmb{w}| x, y) = \\argmax_{\\pmb{w}} \\frac{p(y| \\pmb{w}, x)p(\\pmb{w})}{p(y| x)} = \\argmax_{\\pmb{w}} p(y| \\pmb{w}, x)p(\\pmb{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
